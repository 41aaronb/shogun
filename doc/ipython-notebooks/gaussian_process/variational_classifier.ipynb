{
 "metadata": {
  "name": "",
  "signature": "sha256:d8d617fbc2bcfb1a178836a677e1cb59c55c0d7722d486d95d7ff3c4850a5cca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Variational Inference in the GP modular of Shogun"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "By Wu Lin - <a href=\"mailto:yorker.lin@gmail.com\">yorker.lin@gmail.com</a> - <a href=\"https://github.com/yorkerlin\">https://github.com/yorkerlin</a> \n",
      "\n",
      "\n",
      "Based on the notebook of Gaussian process by Heiko Strathmann - <a href=\"mailto:heiko.strathmann@gmail.com\">heiko.strathmann@gmail.com</a> - <a href=\"https://github.com/karlnapf\">github.com/karlnapf</a> - <a href=\"http://herrstrathmann.de\">herrstrathmann.de</a>, and the GP framework of the Google summer of code 2014 project of Wu Lin,  -  <a href=\"http://www.google-melange.com/gsoc/project/google/gsoc2013/votjak/8001\">Google summer of code 2013 project</a> of Roman Votyakov - <a href=\"mailto:votjakovr@gmail.com\">votjakovr@gmail.com</a> - <a href=\"https://github.com/votjakovr\">github.com/votjakovr</a>, and the <a href=\"http://www.google-melange.com/gsoc/project/google/gsoc2012/walke434/39001\">Google summer of code 2012 project</a> of Jacob Walker - <a href=\"mailto:walke434@gmail.com\">walke434@gmail.com</a> - <a href=\"https://github.com/puffin444\">github.com/puffin444</a> "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is about the <a href=\"http://en.wikipedia.org/wiki/Variational_Bayesian_methods\">variational inference</a> method used in <a href=\"http://en.wikipedia.org/wiki/Statistical_classification\">classification</a> models with <a href=\"http://en.wikipedia.org/wiki/Gaussian_process\">Gaussian Process (GP)</a> priors, which is called Gaussian Process Classification (GPC) models, in Shogun. \n",
      "\n",
      "We assume reader has some background in <a href=\"http://en.wikipedia.org/wiki/Bayesian_statistics\">Bayesian statistics</a>. For background in Bayesian statistics, please see the <a href=\"http://www.shogun-toolbox.org/static/notebook/current/gaussian_processes.html\">notebook</a> about Gaussian Process.\n",
      "\n",
      "After providing a semi-formal introduction, we illustrate how to do training, make prediction, and automatically learn parameters in Shogun"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# import all shogun classes\n",
      "from modshogun import *\n",
      "\n",
      "# import all required libraries\n",
      "import scipy\n",
      "import scipy.io\n",
      "import numpy as np\n",
      "from math import exp,sqrt\n",
      "import time\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Brief Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In binary classification, we get binary labels in form of a column vector, $\\mathbf{y}\\in\\mathcal{Y}^n=\\{-1,+1\\}^n$, and features as a matrix, $\\mathbf{X}\\in\\mathcal{R}^{n\\times d}$, where $n$ is the number of data points and $d$ is the number of features.  A likelihood function $p(\\mathbf{y}|\\text{f},\\mathbf{X})=p(\\mathbf{y}|\\mathbf{f})$ is used to model data with labels, where a function $\\text{f}:\\mathcal{R}^{d}\\to \\mathcal{R} $ drawn from a Gaussian Process prior and $\\text{f}(\\mathbf{X})=\\mathbf{f} \\in \\mathcal{R}^{n}$ is a column vector by applying f to each row of $\\mathbf{X}$. Note that the difference between f and $\\mathbf{f}$ is that f is a function while $\\mathbf{f}$ is a set of point. $p(\\text{f})$ and $p(\\mathbf{f})$ are also different because $p(\\text{f})$ is measured in infinite dimensional (function) space while $p(\\mathbf{f})$ is measured in finite dimensional space.\n",
      "\n",
      "Given data with labels, our goal is to train a Gaussian Process classifier to fit the data well.\n",
      "In other words, we want to learn posterior, $p(\\text{f}|\\mathbf{y},\\mathbf{X})$, given training data points. In fact, we will see in the next session that all we need is to learn $p(\\mathbf{f}|\\mathbf{y})$ (Note that we use $\\mathbf{f}$ as $\\text{f}(\\mathbf{X})$ for short).\n",
      "\n",
      "The key intuition of variational inference is to approximate the distribution of interest (here $p(\\mathbf{f}|\\mathbf{y})$, which is a non-Gaussian distribution) with a more tractable distribution (here a Gaussian $q(\\mathbf{f}|\\mathbf{y}))$, via minimizing the <a href=\"http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\">Kullback\u2013Leibler divergence</a> (KL divergence) \n",
      "\n",
      "${\\mathrm{KL}}(Q\\|P) = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{p(\\mathbf{f}|\\mathbf{y})}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$\n",
      "\n",
      "between the true distribution and the approximation.\n",
      "\n",
      "Please see the next session if you want to know the reason why non-Gaussian $p(\\mathbf{f}|\\mathbf{y})$ cannot be used in inference process of GPC.\n",
      "\n",
      "Throughout this notebook, we deal with binary classification using <a href=\"http://en.wikipedia.org/wiki/Logit\">inverse-logit</a>, (also known as Bernoulli-logistic function)\n",
      "likelihood to model labels, given by \n",
      "\n",
      "$p(\\mathbf{y}|\\mathbf{f})=\\prod_{i=1}^n p(y_\\text{i}|\\text{f}(\\mathbf{X_\\text{i}}))=\\prod_{i=1}^n \\frac{1}{1-\\exp(-y_\\text{i} \\mathbf{f}_\\text{i})}$,\n",
      "where $\\mathbf{X_i}$ is the i-th row of $\\mathbf{X}$ and $\\text{f}(\\mathbf{X_\\text{i}})=\\mathbf{f_\\text{i}}$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "More detailed information about GPC (Skip if you just want code examples)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In GPC models, our goal is to predict the label ($y_{new}$) of a new data point, a column vector, $\\mathbf{x_{new}}\\in\\mathcal{R}^{d}$ based on $p(y_{new}|\\mathbf{y},\\mathbf{X},\\mathbf{x_{new}})$ \n",
      "\n",
      "According to <a href=\"http://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes' theorem</a>, we know that\n",
      "$p(y_{new}|\\mathbf{y},\\mathbf{X},\\mathbf{x_{new}})= \\int {p(y_{new}|f,\\mathbf{x_{new}})p(f|\\mathbf{y},\\mathbf{X}) df}$\n",
      "\n",
      "Informally, according to <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions\">the property</a> of GP about marginalization,\n",
      "\n",
      "$\\int {p(\\mathbf{y_\\text{new}}|\\text{f},\\mathbf{x_\\text{new}})p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}= \\int {p(\\mathbf{y_\\text{new}}|\\mathbf{f_\\text{new}})p(\\mathbf{f_\\text{new}}|\\mathbf{y},\\mathbf{X}) d\\mathbf{f_\\text{new}}}$.\n",
      "\n",
      "where $\\text{f}(\\mathbf{x_\\text{new}})=\\mathbf{f_\\text{new}}$, $p(\\mathbf{y_\\text{new}}|\\mathbf{f_\\text{new}})=p(\\mathbf{y_\\text{new}}|\\text{f},\\mathbf{x_{new}})$ and $p(\\mathbf{f_{new}}|\\mathbf{y},\\mathbf{X})=p(\\mathbf{f_\\text{new}}|\\mathbf{y},\\mathbf{X}, \\mathbf{x_\\text{new}})$ .\n",
      "\n",
      "The key difference here is that $p(f|\\mathbf{y}, \\mathbf{X})$ is measured in infinite dimensional space while $p(\\mathbf{f_{new}}|\\mathbf{y},\\mathbf{X})$ is measured in one-dimensional space.\n",
      "\n",
      "Note that\n",
      "\n",
      "$p(\\mathbf{f_\\text{new}}|\\mathbf{y},\\mathbf{X})=\\int {p(\\mathbf{f_\\text{new}}|\\text{f}) p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}$\n",
      "\n",
      "Similarly, $\\int {p(\\mathbf{f_\\text{new}}|\\text{f}) p(\\text{f}|\\mathbf{y},\\mathbf{X}) d\\text{f}}=\\int {p(\\mathbf{f_\\text{new}}|\\mathbf{f}) p(\\mathbf{f}|\\mathbf{y}) d\\mathbf{f}}$\n",
      "\n",
      "Again, $p(\\mathbf{f}|\\mathbf{y})=p(\\mathbf{f}|\\mathbf{y}, \\mathbf{X})$ is measured in n-dimensional space while $p(\\text{f}|\\mathbf{y},\\mathbf{X})$ is measured in infinite dimensional space.\n",
      "\n",
      "Informally, according to GP the following holds:\n",
      "\n",
      "$p(\\mathbf{f_\\text{new}}|\\mathbf{f})=\\frac{p(\\text{f}(\\mathbf{[\\mathbf{X};\\mathbf{x_\\text{new}}^T]}))}{p(\\text{f}(\\mathbf{\\mathbf{X}}))}$ is followed by finite-dimensional Gaussian distribution, where $[\\mathbf{X};\\mathbf{x_\\text{new}}^T]\\in \\mathcal{R}^{(n+1)\\times d}$ and $\\text{f}(\\mathbf{[\\mathbf{X};\\mathbf{x_\\text{new}}^T]}) \\in \\mathcal{R}^{n+1} $\n",
      "\n",
      "Note that If $p(\\mathbf{f}|\\mathbf{y})$ is followed by a Gaussian distribution, $p(\\mathbf{f_{new}}|\\mathbf{y},\\mathbf{X})$ has a close form.\n",
      "\n",
      "However, in classification $p(\\mathbf{f}|\\mathbf{y})$ usually is NOT followed by Gaussian distribution since\n",
      "\n",
      "$p(\\mathbf{f}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})$, where $p(\\mathbf{f})$ is followed by Gaussian distribution but $p(\\mathbf{y}|\\mathbf{f}))$ (the likelihood) is NOT followed by Gaussian distribution.\n",
      "\n",
      "Variational inference in GPC is to approximate $p(\\mathbf{f}|\\mathbf{y})$ using a Gaussian distribution, $q(\\mathbf{f}|\\mathbf{y})$, via minimizing the KL divergence, ${\\mathrm{KL}}(Q\\|P)$.\n",
      "\n",
      "Reader may note that KL divergence is asymmetric. If we minimizing ${\\mathrm{KL}}(P\\|Q)$ instead of ${\\mathrm{KL}}(Q\\|P)$, it is about inference using <a href=\"http://en.wikipedia.org/wiki/Expectation_propagation\">expectation propagation</a> (EP) in GPC. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variational Inference in GPC (Skip if you just want code examples)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on the first session, variaitonal inference in GPC is about **Minimizing** the KL divergence listed as below:\n",
      "\n",
      "${\\mathrm{KL}}(Q\\|P) = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{p(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} = \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{  \\frac{p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})}{p(\\mathbf{y})}  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}=\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f}|\\mathbf{y})  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} - \\text{const}$. \n",
      "\n",
      "\n",
      "Another way to explain variational inference in GPC is about **Maximizing** a lower bound of the log of marginal likelihood, $ln (p(\\mathbf{y}|\\mathbf{X})) $.\n",
      "\n",
      "$ln (p(\\mathbf{y}|\\mathbf{X})) = ln (\\int_{-\\infty}^\\infty {p(\\mathbf{y}|\\text{f})p(\\text{f}|\\mathbf{X})} d\\text{f}) =  ln (\\int_{-\\infty}^\\infty {p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} d\\mathbf{f})= ln (\\int_{-\\infty}^\\infty { q(\\mathbf{f}) \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} d\\mathbf{f}) \\geq  (\\int_{-\\infty}^\\infty { q(\\mathbf{f}|\\mathbf{y}) ln ( \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} ) d\\mathbf{f})$\n",
      "\n",
      "\n",
      "where the inequality is based on <a href=\"http://en.wikipedia.org/wiki/Jensen%27s_inequality\">Jensen\u2019s inequality</a>.\n",
      "\n",
      "$\\int_{-\\infty}^\\infty { q(\\mathbf{f}|\\mathbf{y}) ln ( \\frac{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})} {q(\\mathbf{f}|\\mathbf{y})}} ) d\\mathbf{f}=- \\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{ p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})  }\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f} = -E_q[ln(q(\\mathbf{f}|\\mathbf{y}))] + E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))]$, \n",
      "\n",
      "where $E_q(\\cdot)$ is the expectation with respect to $\\mathbf{f}$.\n",
      "\n",
      "Note that the last term, $E_q[ln(p(\\mathbf{y}|\\mathbf{f}))]$, in GPC usually does NOT have a close form.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dealing with the non-close form issue in GPC (Skip if you just want code examples)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can show that $E_q[ln(p(\\mathbf{y}|\\mathbf{f}))]$ can be expressed in term of summation of one-dimensional integrations via the similar <a href=\"http://en.wikipedia.org/wiki/Multivariate_normal_distribution#Marginal_distributions\">marginalization property</a> for multivariate Gaussian distribution. \n",
      " \n",
      "$E_q[ln(p(\\mathbf{y}|\\mathbf{f}))]=E_q[\\sum_{i=1}^n {ln(p(\\mathbf{y_\\text{i}}|\\mathbf{f_\\text{i}})}]=\\sum_{i=1}^n {E_q[ln(p(\\mathbf{y_i}|\\mathbf{f_\\text{i}})]}=\\sum_{i=1}^n {E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_\\text{i}})]}$\n",
      "\n",
      "where $q$ denotes $q(\\mathbf{f}|\\mathbf{y})$ followed by multivariable $N(\\mu, \\Sigma)$, $q_i$ denotes $q_i(\\mathbf{f_\\text{i}}|\\mathbf{y_i})$ followed by univariate $N(\\mu_i, \\Sigma_{i,i})$, and $\\mu_i$ and $\\Sigma_{i,i}$ are the i-th element of the mean $\\mu$ and the i-th diagonal element of the covariance matrix $\\Sigma$ of $q(\\mathbf{f}|\\mathbf{y})$ respectively.\n",
      "\n",
      "Of course, $E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_i})]$ usually does not have a close form in GPC.\n",
      "One way to deal with this issue is one dimensional numerical integration.\n",
      "Another way is using some variational bounds.\n",
      "\n",
      "For now, we assume this expectation is given and we will briefly discuss how to approximate it in later session."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A toy example to visually explain variational inference via existing variaitonal methods in Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, variational inference in GPC is to approximate $p(\\mathbf{f}|\\mathbf{y})$ using a Gaussian distribution, $q(\\mathbf{f}|\\mathbf{y})$.\n",
      "\n",
      "Note that $p(\\mathbf{f}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{f})p(\\mathbf{f})$, where $p(\\mathbf{f})$ is the prior and $p(\\mathbf{y}|\\mathbf{f}))$ is the likelihood.\n",
      "\n",
      "\n",
      "In this toy example, the prior is followed by Gaussian distribution and the likelihood is followed by Bernoulli-logistic.\n",
      "\n",
      "Our goal is to approximate the posterior,$p(\\mathbf{f}|\\mathbf{y})$, using a Gaussian distrbution, $q(\\mathbf{f}|\\mathbf{y})$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Gaussian_points(Sigma,mu,xmin,xmax,ymin,ymax,delta=0.01):\n",
      "    xlist                    = np.arange(xmin, xmax, delta)\n",
      "    ylist                    = np.arange(ymin, ymax, delta)\n",
      "    X, Y                     = np.meshgrid(xlist, ylist)\n",
      "    model                    = GaussianDistribution(mu, Sigma)\n",
      "    Z = []\n",
      "    for items in zip(X,Y):\n",
      "        for sample in zip(items[0],items[1]):\n",
      "            sample = np.asarray(sample)\n",
      "            Z.append(model.log_pdf(sample))\n",
      "    Z = np.asarray(Z).reshape(len(X),len(Z)/len(X))\n",
      "    return (X,Y,Z)\n",
      "\n",
      "def likelihood_points(X,Y,labels,likelihood):\n",
      "    Z = []\n",
      "    for items in zip(X,Y):\n",
      "        for sample in zip(items[0],items[1]):\n",
      "            sample = np.asarray(sample)\n",
      "            lpdf   = likelihood.get_log_probability_f(labels, sample).sum()\n",
      "            Z.append(lpdf)\n",
      "    Z = np.asarray(Z).reshape(len(X),len(Z)/len(X))\n",
      "    return Z\n",
      "\n",
      "def approx_posterior_plot(methods, kernel_func, features, mean_func, labels, likelihood, kernel_log_scale, \n",
      "                          xmin, xmax, ymin, ymax, delta, plots):\n",
      "    (rows, cols) = plots.shape\n",
      "    methods=np.asarray(methods).reshape(rows, cols)\n",
      "    for r in xrange(rows):\n",
      "        for c in xrange(cols):\n",
      "            inference                = methods[r][c]\n",
      "            inf                      = inference(kernel_func, features, mean_func, labels, likelihood)\n",
      "            inf.set_scale(exp(kernel_log_scale))\n",
      "            #get the approximated Gaussian distribution\n",
      "            mu                       = inf.get_posterior_mean()\n",
      "            Sigma                    = inf.get_posterior_covariance()\n",
      "            #normalized approximated posterior\n",
      "            (X,Y,Z)                  = Gaussian_points(Sigma, mu, xmin, xmax, ymin, ymax, delta)\n",
      "            plots[r][c].contour(X, Y, np.exp(Z))\n",
      "            plots[r][c].set_title('posterior via %s'%inf.get_name())\n",
      "            plots[r][c].axis('equal')\n",
      "        \n",
      "x=np.asarray([sqrt(2),-sqrt(2)]).reshape(1,2)\n",
      "y=np.asarray([1,-1])\n",
      "\n",
      "features                 = RealFeatures(x)\n",
      "labels                   = BinaryLabels(y)\n",
      "kernel_log_sigma         = 1.0\n",
      "kernel_log_scale         = 1.5\n",
      "\n",
      "mean_func                = ConstMean()\n",
      "kernel_sigma             = 2*exp(2*kernel_log_sigma)\n",
      "kernel_func              = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "kernel_func.init(features, features)\n",
      "\n",
      "Sigma                    = kernel_func.get_kernel_matrix()\n",
      "Sigma                    = Sigma * exp(2.0*kernel_log_scale)\n",
      "mu                       = mean_func.get_mean_vector(features)\n",
      "\n",
      "delta                    = 0.1\n",
      "xmin                     = -4\n",
      "xmax                     = 6\n",
      "ymin                     = -6\n",
      "ymax                     = 4\n",
      "#prior (Gaussian)\n",
      "(X,Y,Z1)                 = Gaussian_points(Sigma, mu, xmin, xmax, ymin, ymax, delta)\n",
      "\n",
      "col_size                 = 6\n",
      "f, (ax1, ax2, ax3) =plt.subplots(1, 3, figsize=(col_size*3,col_size))\n",
      "\n",
      "ax1.contour(X, Y, np.exp(Z1))\n",
      "ax1.set_title('prior')\n",
      "ax1.axis('equal')\n",
      "\n",
      "#likelihood (inverse logit)\n",
      "likelihood               = LogitDVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "\n",
      "Z2                       = likelihood_points(X,Y,labels,likelihood)\n",
      "ax2.contour(X, Y, np.exp(Z2))\n",
      "ax2.set_title('likelihood')\n",
      "ax2.axis('equal')\n",
      "\n",
      "#unnormalized true posterior (non-Gaussian)\n",
      "Z3                       = Z1+Z2\n",
      "ax3.contour(X, Y, np.exp(Z3))\n",
      "ax3.set_title('true posterior')\n",
      "ax3.axis('equal')\n",
      "\n",
      "f, plots =plt.subplots(2, 3, figsize=(col_size*3,col_size*2))\n",
      "\n",
      "methods=[\n",
      "SingleLaplacianInferenceMethod,\n",
      "SingleLaplacianInferenceMethodWithLBFGS,\n",
      "KLApproxDiagonalInferenceMethod,\n",
      "KLCovarianceInferenceMethod,\n",
      "KLCholeskyInferenceMethod,\n",
      "KLDualInferenceMethod\n",
      "]\n",
      "\n",
      "approx_posterior_plot(methods, kernel_func, features, mean_func, labels, likelihood, kernel_log_scale, \n",
      "                      xmin, xmax, ymin, ymax, delta, plots)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "A toy example to demonstrate the usage of variational methods in Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We apply variational methods to the sonar data set, which can be found at <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/\">here</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def learning(inference, linesearch, x_train, x_test, y_train, y_test, kernel_log_sigma, kernel_log_scale):\n",
      "\n",
      "    likelihood               = LogitDVGLikelihood()\n",
      "    likelihood.set_noise_factor(1e-15)\n",
      "    likelihood.set_strict_scale(0.01)\n",
      "    #error_eval               = ErrorRateMeasure()\n",
      "    mean_func                = ZeroMean()\n",
      "    kernel_sigma             = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func              = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #sample by 1\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #feature by sample\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    #print \"\\nusing %s\"%inf.get_name()\n",
      "    \n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "        inf.set_lbfgs_parameters(100,400,int(linesearch),400)\n",
      "        inf.set_noise_factor(1e-3)\n",
      "        inf.set_min_coeff_kernel(1e-2)\n",
      "        inf.set_max_attempt(10)\n",
      "    except:\n",
      "        pass\n",
      "\n",
      "    gp = GaussianProcessClassification(inf)\n",
      "    #start = time.time()\n",
      "    gp.train()\n",
      "    #end = time.time()\n",
      "    #print \"cost %s seconds at training\"%(end-start)\n",
      "\n",
      "    #nlz=inf.get_negative_log_marginal_likelihood()\n",
      "    #start = time.time()\n",
      "    #classification on train_data\n",
      "    #pred_labels_train = gp.apply_binary(features_train)\n",
      "    #classification on test_data\n",
      "    #pred_labels_test = gp.apply_binary(features_test)\n",
      "    #end = time.time()    \n",
      "    #print \"cost %s seconds at prediction\"%(end-start)\n",
      "    \n",
      "    #error_train = error_eval.evaluate(pred_labels_train, labels_train)\n",
      "    #error_test = error_eval.evaluate(pred_labels_test, labels_test)\n",
      "    #print \"Train error : %.4f Test error: %.4f\\n\" % (error_train, error_test);\n",
      "\n",
      "    prob=gp.get_probabilities(features_test)\n",
      "\n",
      "    return prob, inf.get_name()\n",
      "    \n",
      "\n",
      "import random\n",
      "labels={\n",
      "    \"m\":1,\n",
      "    \"r\":-1,\n",
      "       }\n",
      "from math import log\n",
      "def extract(inf, train_size):\n",
      "    random.seed(1)\n",
      "    x=[]\n",
      "    y=[]\n",
      "    for line in open(inf):\n",
      "        line=line.strip()\n",
      "        info=line.split(',')\n",
      "        label=labels[info[-1].lower()]\n",
      "        x.append(map(float, info[:-1]))\n",
      "        y.append(label)\n",
      "    \n",
      "    assert train_size < len(x)\n",
      "    idx=range(len(y))\n",
      "    random.shuffle(idx)\n",
      "    train_idx=set(idx[:train_size])\n",
      "    test_idx=set(idx[train_size:])\n",
      "    x_train = np.asarray([value for (idx, value) in enumerate(x) if idx in train_idx]).T\n",
      "    y_train = np.asarray([label for (idx, label) in enumerate(y) if idx in train_idx])\n",
      "    x_test = np.asarray([value for (idx, value) in enumerate(x) if idx in test_idx]).T\n",
      "    y_test = np.asarray([label for (idx, label) in enumerate(y) if idx in test_idx])\n",
      "\n",
      "    y_train_positive=(y_train==1).sum()\n",
      "    y_train_negative=(y_train==-1).sum()\n",
      "    y_test_positive=(y_test==1).sum()\n",
      "    y_test_negative=(y_test==-1).sum()\n",
      "\n",
      "    assert y_test_positive+y_test_negative == len(y_test)\n",
      "    assert y_train_positive+y_train_negative == len(y_train)\n",
      "\n",
      "    prb_positive=float(y_train_positive)/len(y_train)\n",
      "    B=float(y_test_positive)*log(prb_positive,2)+float(y_test_negative)*log(1.0-prb_positive,2)\n",
      "    B=-B/len(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test, B\n",
      "\n",
      "def approx_bit_plot(methods, linesearchs, plots, lScale, lSigma):\n",
      "    if len(plots.shape)==1:\n",
      "        rows=1\n",
      "        cols=plots.shape[0]\n",
      "    else:\n",
      "        (rows, cols) = plots.shape\n",
      "\n",
      "    methods=np.asarray(methods).reshape(rows, cols)\n",
      "    linesearchs=np.asarray(linesearchs).reshape(rows, cols)\n",
      "\n",
      "    for r in xrange(rows):\n",
      "        for c in xrange(cols):\n",
      "            inference                = methods[r][c]\n",
      "            linesearch               = linesearchs[r][c]\n",
      "            scores=[]\n",
      "            for items in zip(lScale, lSigma):\n",
      "                for parameters in zip(items[0],items[1]):\n",
      "                    lscale=parameters[0]\n",
      "                    lsigma=parameters[1]\n",
      "                    (prob, inf_name)=learning(inference, linesearch, x_train, x_test, y_train, y_test, lscale, lsigma)\n",
      "                    #print parameters, inf_name, linesearch\n",
      "                    score=0.0\n",
      "                    for (label_idx, prb_idx) in zip(y_test, prob):\n",
      "                        if label_idx==1:\n",
      "                            assert prb_idx>1e-15\n",
      "                            score+=(1.0+label_idx)*log(prb_idx,2)\n",
      "                        elif label_idx==-1:\n",
      "                            assert (1.0-prb_idx)>1e-15\n",
      "                            score+=(1.0-label_idx)*log(1.0-prb_idx,2)\n",
      "                        else:\n",
      "                            score+=(1.0+label_idx)*log(prb_idx,2)+(1.0-label_idx)*log(1.0-prb_idx,2)\n",
      "                    score=score/(2.0*len(y_test))+B\n",
      "                    scores.append(score)\n",
      "            scores=np.asarray(scores).reshape(len(lScale),len(scores)/len(lScale))\n",
      "            #print inf_name, linesearch\n",
      "\n",
      "            if len(plots.shape)==1:\n",
      "                sub_plot=plots\n",
      "            else:\n",
      "                sub_plot=plots[r]\n",
      "            CS =sub_plot[c].contour(lScale, lSigma, scores)\n",
      "            sub_plot[c].clabel(CS, inline=1, fontsize=10)\n",
      "            sub_plot[c].set_title('information bit for %s'%inf_name)\n",
      "            sub_plot[c].axis('equal')\n",
      "\n",
      "train_size=108\n",
      "(x_train, y_train, x_test, y_test, B)=extract('./sonar.all-data', train_size)\n",
      "inference_methods=[\n",
      "                  EPInferenceMethod,\n",
      "                  SingleLaplacianInferenceMethod,\n",
      "                  KLApproxDiagonalInferenceMethod,\n",
      "                  KLCholeskyInferenceMethod,\n",
      "                  KLCovarianceInferenceMethod,\n",
      "                  KLDualInferenceMethod,\n",
      "                  ]\n",
      "linesearchs=[\n",
      "            3,\n",
      "            3,\n",
      "            3,\n",
      "            3,\n",
      "            3,\n",
      "            1,\n",
      "            ]\n",
      "\n",
      "col_size=8\n",
      "lscale_min=0.0\n",
      "lscale_max=5.0\n",
      "lsigma_min=0.0\n",
      "lsigma_max=5.0\n",
      "delta=0.2\n",
      "\n",
      "lscale_list              = np.arange(lscale_min, lscale_max, delta)\n",
      "lsigma_list              = np.arange(lsigma_min, lsigma_max, delta)\n",
      "lScale, lSigma           = np.meshgrid(lscale_list, lsigma_list)\n",
      "f, plots =plt.subplots(2, 3, figsize=(col_size*3,col_size*2))\n",
      "\n",
      "\n",
      "approx_bit_plot(inference_methods, linesearchs, plots, lScale, lSigma)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the x axis and y axis in these figures are the range of parameter of covariance function.\n",
      "The information bit is used to measure the accuracy of classification. \n",
      "If there is a perfect classification, the information bit should be 1."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Another toy example to demonstrate the usage of variational methods in Shogun"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the US Postal Service(USPS) database of handwritten digits as an example in this notebook.\n",
      "\n",
      "We consider to classify the images of digit 3 from images of digit 5 as an example, which is used in the <a href=\"http://www.gaussianprocess.org/gpml/\">Gaussian Processes for Machine Learning</a> textbook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#extract trainning set and test set from the dataset\n",
      "def binary_extract(idx, features, labels):\n",
      "        #binary classification\n",
      "        assert len(idx) == 2\n",
      "        positive_idx = (labels[idx[0],:] == 1)\n",
      "        negative_idx = (labels[idx[1],:] == 1)\n",
      "        binary_idx = (positive_idx | negative_idx)\n",
      "        ds = binary_idx.shape[-1]\n",
      "\n",
      "        bin_labels = np.zeros(ds)\n",
      "        bin_labels[positive_idx] = 1\n",
      "        bin_labels[negative_idx] = -1\n",
      "\n",
      "        binary_features = (features[:,binary_idx])\n",
      "        binary_labels = (bin_labels[binary_idx])\n",
      "        #number of sample should be the same for features and lables\n",
      "        assert binary_features.shape[-1] == binary_labels.shape[-1]\n",
      "\n",
      "        positive_count = bin_labels[positive_idx].shape[-1]\n",
      "        negative_count = bin_labels[negative_idx].shape[-1]\n",
      "        binary_count = binary_labels.shape[-1]\n",
      "        assert positive_count + negative_count == binary_count\n",
      "        #print out the number of negative and positive sample\n",
      "        print \"positive samples %d negative samples %d total samples %d\" %(positive_count, negative_count, binary_count)\n",
      "        return binary_features, binary_labels\n",
      "\n",
      "def learning(inference, likelihood, x_train, x_test, y_train, y_test):\n",
      "    error_eval               = ErrorRateMeasure()\n",
      "\n",
      "    mean_func                = ConstMean()\n",
      "    kernel_log_sigma         = 1.0\n",
      "    kernel_sigma             = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func              = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #sample by 1\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #feature by sample\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    kernel_log_scale = 1.0\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    print \"\\nusing %s\"%inf.get_name()\n",
      "    \n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "            inf.set_lbfgs_parameters(100,80,0,80)\n",
      "    except:\n",
      "            pass\n",
      "\n",
      "    start = time.time()\n",
      "    gp = GaussianProcessBinaryClassification(inf)\n",
      "    gp.train()\n",
      "    end = time.time()\n",
      "    print \"cost %s seconds at training\"%(end-start)\n",
      "    nlz=inf.get_negative_log_marginal_likelihood()\n",
      "    print \"the negative_log_marginal_likelihood is %.4f\"%nlz\n",
      "    start = time.time()\n",
      "    #classification on train_data\n",
      "    pred_labels_train = gp.apply_binary(features_train)\n",
      "    #classification on test_data\n",
      "    pred_labels_test = gp.apply_binary(features_test)\n",
      "    end = time.time()    \n",
      "    print \"cost %s seconds at prediction\"%(end-start)\n",
      "    \n",
      "    error_train = error_eval.evaluate(pred_labels_train, labels_train)\n",
      "    error_test = error_eval.evaluate(pred_labels_test, labels_test)\n",
      "    \n",
      "    print \"Train error : %.4f Test error: %.4f\\n\" % (error_train, error_test);\n",
      "    \n",
      "inf='./usps_resampled.mat'\n",
      "data=scipy.io.loadmat(inf)\n",
      "train_labels=data['train_labels']\n",
      "test_labels=data['test_labels']\n",
      "train_features=data['train_patterns']\n",
      "test_features=data['test_patterns']\n",
      "#using images of digit 3 and digit 5 from the dataset\n",
      "idx=[3,5]\n",
      "#Note that \n",
      "#y_train and y_test are followed the definition in the first session \n",
      "#the transpose of x_train and x_test are followed the definition in the first session\n",
      "print \"Training set statistics\"\n",
      "(x_train, y_train)=binary_extract(idx,train_features, train_labels)\n",
      "print \"Test set statistics\"\n",
      "(x_test, y_test)=binary_extract(idx,test_features, test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Laplace Method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://en.wikipedia.org/wiki/Laplace%27s_method\">Laplace method</a> in GPC can be viewed as a special case of variational method.\n",
      "\n",
      "The idea of Laplace method is to use the second-order <a href=\"http://en.wikipedia.org/wiki/Taylor_series\">Taylor approximation</a> in the <a href=\"http://en.wikipedia.org/wiki/Mode_%28statistics%29\">mode</a>, $\\mathbf{\\hat{f}}$ of $p(\\mathbf{f}|\\mathbf{y})$ to approximate $p(\\mathbf{f}|\\mathbf{y})$, which is given below:\n",
      "\n",
      "$p(\\mathbf{f}|\\mathbf{y}) \\approx \\hat{p}(\\mathbf{f}|\\mathbf{y})$ followed by $N(\\mathbf{\\hat{f}}, H_{\\mathbf{\\hat{f}}}^{-1})$ ,where $H_{\\mathbf{\\hat{f}}}$ is the <a href=\"http://en.wikipedia.org/wiki/Hessian_matrix\">Hessian matrix</a> in $\\mathbf{\\hat{f}}$\n",
      "\n",
      "Therefore, the KL divergence, ${\\mathrm{KL}}(Q\\|P)$, is approximated by $\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{\\hat{p}(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$.\n",
      "\n",
      "**Minimizing** $\\int_{-\\infty}^\\infty \\ln\\left(\\frac{q(\\mathbf{f}|\\mathbf{y})}{\\hat{p}(\\mathbf{f}|y)}\\right) q(\\mathbf{f}|\\mathbf{y}) \\ {\\rm d}\\mathbf{f}$\n",
      "\n",
      "we can get $q(\\mathbf{f}|\\mathbf{y})= \\hat{p}(\\mathbf{f}|\\mathbf{y})$.\n",
      "\n",
      "In practice, we can use <a href=\"http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\">Newton-Raphson</a> optimizer or <a href=\"http://en.wikipedia.org/wiki/Quasi-Newton_method\">Quasi-Newton</a> optimizers(ie, <a href=\"http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\">Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno</a> (LBFGS)) to find $\\mathbf{\\hat{f}}$.\n",
      "\n",
      "Since the likelihood is inverse-logit, we can show that the objective function, $ln(p(\\mathbf{f}|\\mathbf{y}))$, is strictly <a href=\"http://en.wikipedia.org/wiki/Concave\">concave</a>, which means in theory these optimizers can find the same global solution $\\mathbf{\\hat{f}}$.\n",
      "\n",
      "We demonstrate how to apply Laplace method via Newton-Raphson optimizer and LBFGS optimizer in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inference_methods=[\n",
      "                  SingleLaplacianInferenceMethodWithLBFGS,     #using LBFGS optimizer\n",
      "                  SingleLaplacianInferenceMethod,              #using Newton-Raphson optimizer\n",
      "                  ]\n",
      "likelihood               = LogitLikelihood()\n",
      "for inference in inference_methods:\n",
      "    learning(inference, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: Laplace method is the fastest method among all implemented variational methods in Shogun in practice if Laplace method is considered as a special case of variational inference. However, the second-order Taylor approximation in the mode of marginal likelihood is not always a good approximation, which can be observed in the figures in the previous session for the sonar data set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Covariance variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is mentioned in <a href=\"http://jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf\">the paper</a> of Nickisch and Rasmussen in 2008, which is called the KL method in the paper.\n",
      "\n",
      "The idea of this method is to maximize $E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[ln(q(\\mathbf{f}|\\mathbf{y}))]$ with respect to $\\mu$ and $\\Sigma$, by reparametrizing $\\Sigma$ to reduce the dimension of variable to be optimized.\n",
      "\n",
      "However, such parametrization does not preserve convexity (concave in this setting) according to <a href=\"http://arxiv.org/abs/1306.1052\">the paper</a> of Khan et. al. in 2013. \n",
      "Except for this method, convexity holds in other variational methods implemented in Shogun. \n",
      "\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood               = LogitVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "learning(KLCovarianceInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method may be the first variational method used in GPC. However, it is the slowest method among all implemented variational methods in Shogun in practice."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Mean-field variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is also called factorial variational method in <a href=\"http://jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf\">the paper</a> of Nickisch and Rasmussen in 2008.\n",
      "\n",
      "The idea of mean-field variatonal method is to enforce artificial structure on the co-variance matrix, $\\Sigma$ of $q(\\mathbf{f})$.\n",
      "\n",
      "In mean-field variatonal method, $\\Sigma$ is a diagonal positive-definite matrix.\n",
      "\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood               = LogitVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "learning(KLApproxDiagonalInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method could be as fast as the Laplace method in Shogun in practice but ignores all off-diagonal elements of the covariance matrix. However, in some case (ie, there are a lot of noise in data set), such structure regularization may bring some promising result compared to other variational methods, which can be observed in the figures in the previous session for the sonar data set."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Cholesky variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is proposed in <a href=\"http://jmlr.org/proceedings/papers/v15/challis11a/challis11a.pdf\">the paper</a> of Challis and Barber in 2011.\n",
      "\n",
      "The idea of this method is to maximize $E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[ln(q(\\mathbf{f}|\\mathbf{y}))]$ in terms of the Cholesky representation, C, for the covariance matrix of $q(\\mathbf{f})$, where $\\Sigma=CC^T$ and C is a lower triangular matrix.\n",
      "\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood               = LogitVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "learning(KLCholeskyInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method may be faster to learn the complete structure of the covariance matrix from data than covariance variational method.\n",
      "\n",
      "The reason is solving linear system related to $\\Sigma$ is required at each optimization step.\n",
      "\n",
      "In Cholesky representation, the time complexity at each optimization step is reduced to O(n^2) from O(n^3) compared to covariance variational method.\n",
      "\n",
      "However, the overall time complexity is still O(n^3)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Dual variational method in GPC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This method is proposed in <a href=\"http://arxiv.org/abs/1306.1052\">the paper</a> of Khan et. al. in 2013.\n",
      "The idea of this method is to optimize $E_q[ln(p(\\mathbf{f}))] + E_q[ln(p(\\mathbf{y}|\\mathbf{f}))] - E_q[ln(q(\\mathbf{f}|\\mathbf{y}))]$ in <a href=\"http://en.wikipedia.org/wiki/Duality_%28optimization%29\">Lagrange dual</a> form instead of the primal form used in covariance variational method via explicitly expressing constraint in the covariance matrix, $\\Sigma$, in terms of auxiliary variable.\n",
      "\n",
      "However, the time complexity of each optimization step is still O(n^3) and variational bound usually is required to approximate $E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_i})]$ for GPC in this method.\n",
      "\n",
      "We demonstrate how to apply this variational method in Shogun to the USPS data set as below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood               = LogitDVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "likelihood.set_strict_scale(0.1)\n",
      "learning(KLDualInferenceMethod, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remark: This method requires less memory than the Cholesky method, which is important in large-scale case.\n",
      "\n",
      "A promising observation in practice is that this method is faster than covariance variational method and may not be slower than the Cholesky method. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Variational bounds"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we discuss how to approximate $E_{q_i}[ln(p(\\mathbf{y_i}|\\mathbf{f_i})]$.\n",
      "\n",
      "Since this term is about one-dimensional integration, for GPC we can use <a href=\"http://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature\">Gauss\u2013Hermite quadrature</a> to appromxiate this term. In Shogun, the corresponding implementation of Bernoulli-logistic likelihood for variational inference is called CLogitVGLikelihood. This likelihood can be used for all variational methods except the dual variational method.\n",
      "\n",
      "The piecewise variational bound proposed at <a href=\"http://www.icml-2011.org/papers/376_icmlpaper.pdf\">the paper</a> of Benjamin M. Marlin et. al. in 2011 is also implemented in Shogun, which is called CLogitVGPiecewiseBoundLikelihood. This likelihood can be used for all variational methods except the dual variational method.\n",
      "\n",
      "For dual variational method, we use the variational bound in <a href=\"http://www.cs.cmu.edu/~lafferty/pub/ctm.pdf\">the paper</a> of DM Blei et. al. in 2007. The corresponding implementation in Shogun is called CLogitDVGLikelihood. Note that when using CLogitDVGLikelihood, the bound is only enabled for the dual variational method. When other variational methods use this class, it uses one-dimensional integration instead of the variational bound.\n",
      "\n",
      "For detailed discussion about variational bound in GPC, please refer to <a href=\"https://circle.ubc.ca/handle/2429/43640\">Khan's work</a>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likelihood               = LogitVGPiecewiseBoundLikelihood()\n",
      "likelihood.set_default_variational_bound()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "inference_methods=[\n",
      "                  KLCholeskyInferenceMethod,\n",
      "                  KLApproxDiagonalInferenceMethod,\n",
      "                  KLCovarianceInferenceMethod,\n",
      "                  ]\n",
      "for inference in inference_methods:\n",
      "    learning(inference, likelihood, x_train, x_test, y_train, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Optimizing hyper-parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We demonstrate how to optimize hypyer-parameters in Shogun for applying the mean-field variational method to the USPS data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learning2(inference, likelihood, x_train, x_test, y_train, y_test):\n",
      "\n",
      "    error_eval               = ErrorRateMeasure()\n",
      "    mean_func                = ZeroMean()\n",
      "    kernel_log_sigma         = 1.0\n",
      "    kernel_sigma             = 2*exp(2*kernel_log_sigma);\n",
      "    kernel_func              = GaussianKernel(10, kernel_sigma)\n",
      "\n",
      "    #sample by 1\n",
      "    labels_train = BinaryLabels(y_train)\n",
      "    labels_test = BinaryLabels(y_test)\n",
      "    #feature by sample\n",
      "    features_train=RealFeatures(x_train)\n",
      "    features_test=RealFeatures(x_test)\n",
      "\n",
      "    kernel_log_scale = 1.0\n",
      "\n",
      "    inf = inference(kernel_func, features_train, mean_func, labels_train, likelihood)\n",
      "    print \"\\nusing %s\"%inf.get_name()\n",
      "    \n",
      "    inf.set_scale(exp(kernel_log_scale))\n",
      "    try:\n",
      "            inf.set_lbfgs_parameters(100,80,0,80)\n",
      "    except:\n",
      "            pass\n",
      "\n",
      "    gp = GaussianProcessBinaryClassification(inf)\n",
      "\n",
      "    # evaluate our inference method for its derivatives\n",
      "    grad = GradientEvaluation(gp, features_train, labels_train, GradientCriterion(), False)\n",
      "    grad.set_function(inf)\n",
      "\n",
      "    # handles all of the above structures in memory\n",
      "    grad_search = GradientModelSelection(grad)\n",
      "\n",
      "    # search for best parameters and store them\n",
      "    best_combination = grad_search.select_model()\n",
      "\n",
      "    # apply best parameters to GP\n",
      "    best_combination.apply_to_machine(gp)\n",
      "\n",
      "    # we have to \"cast\" objects to the specific kernel interface we used (soon to be easier)\n",
      "    best_width=GaussianKernel.obtain_from_generic(inf.get_kernel()).get_width()\n",
      "    best_scale=inf.get_scale()\n",
      "    print \"Selected kernel bandwidth:\", best_width\n",
      "    print \"Selected kernel scale:\", best_scale\n",
      "\n",
      "    start = time.time()\n",
      "    gp.train()\n",
      "    end = time.time()\n",
      "    print \"cost %s seconds at training\"%(end-start)\n",
      "    nlz=inf.get_negative_log_marginal_likelihood()\n",
      "    print \"the negative_log_marginal_likelihood is %.4f\"%nlz\n",
      "    start = time.time()\n",
      "    #classification on train_data\n",
      "    pred_labels_train = gp.apply_binary(features_train)\n",
      "    #classification on test_data\n",
      "    pred_labels_test = gp.apply_binary(features_test)\n",
      "    end = time.time()    \n",
      "    print \"cost %s seconds at prediction\"%(end-start)\n",
      "    \n",
      "    error_train = error_eval.evaluate(pred_labels_train, labels_train)\n",
      "    error_test = error_eval.evaluate(pred_labels_test, labels_test)\n",
      "    \n",
      "    print \"Train error : %.4f Test error: %.4f\\n\" % (error_train, error_test);\n",
      "    \n",
      "likelihood               = LogitVGLikelihood()\n",
      "likelihood.set_noise_factor(1e-15)\n",
      "learning2(KLApproxDiagonalInferenceMethod, likelihood, x_train, x_test, y_train, y_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Soon to Come"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " * Variational inference for GP multi-classification \n",
      " * Large-scale inference in GPC\n",
      " * Sparse online inference in GPC\n",
      " * Latent GP models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}