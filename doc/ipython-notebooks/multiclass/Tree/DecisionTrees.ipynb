{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decision Trees"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "*By Parijat Mazumdar (GitHub ID: [mazumdarparijat](https://github.com/mazumdarparijat))*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook illustrates the use of [decision trees](http://en.wikipedia.org/wiki/Decision_tree) in Shogun for classification and regression. Various [decision tree learning](http://en.wikipedia.org/wiki/Decision_tree_learning) algorithms like [ID3](http://en.wikipedia.org/wiki/ID3_algorithm), [C4.5](http://en.wikipedia.org/wiki/C4.5_algorithm), [CART](http://themainstreamseer.blogspot.in/2013/01/introduction-to-classification.html), [CHAID](http://en.wikipedia.org/wiki/CHAID) have been discussed in detail using both intuitive toy datasets as well as real-world datasets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Decision Tree Basics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Decision Trees are a non-parametric supervised learning method that can be used for both classification and regression. Decision trees essentially encode a set of if-then-else rules which can be used to predict target variable given data features. These if-then-else rules are formed using the training dataset with the aim to satisfy as many training data instances as possible. The formation of these rules (aka. decision tree) from training data is called decision tree learning. Various decision tree learning algorithms have been developed and they work best in different situations. An advantage of decision trees is that they can model any type of function for classification or regression which other techniques cannot. But a decision tree is highly prone to overfitting and bias towards training data. So, decision trees are used for very large datasets which are assumed to represent the ground truth well. Additionally, certain [tree pruning algorithms](http://en.wikipedia.org/wiki/Pruning_%28decision_trees%29) are also used to tackle overfitting.   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "ID3 (Iterative Dichotomiser 3)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "ID3 is a straightforward decision tree learning algorithm developed by [Ross Quinlan](http://en.wikipedia.org/wiki/Ross_Quinlan). ID3 is applicable only in cases where the attributes (or features) defining data examples are categorical in nature and the data examples belong to pre-defined, clearly distinguishable (ie. well defined) classes. ID3 is an iterative greedy algorithm which starts with the root node and eventually builds the entire tree. At each node, the \"best\" attribute to classify data is chosen. The \"best\" attribute is chosen using the [information gain metric](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees). Once an attribute is chosen in a node, the data examples in the node are categorized into sub-groups based on the attribute values that they have. Basically, all data examples having the same attribute value are put together in the same sub-group. These sub-groups form the children of the present node and the algorithm is repeated for each of the newly formed children nodes. This goes on until all the data members of a node belong to the same class or all the attributes are exhausted. In the latter case, the class predicted may be erroneous and generally the mode of the classes appearing in the node is chosen as the predictive class. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Pseudocode for ID3 Algorithm"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\tInputs: (D, A, C), where:\n",
      "\t\tD is a dataset with only nominal instance attributes A \n",
      "\t\tC is the class attribute\n",
      "\n",
      "\tOutput: a decision tree T representing a sequential decision process for\n",
      "\tclassifying instances (predicting the values of the class attribute C);\n",
      "\teach node of T is labeled with a non-class attribute of A\n",
      "\n",
      "\tInformal Inductive Bias: minimize the average height of the tree\n",
      "\n",
      "\tProcedure:\n",
      "\n",
      "\tif the set of remaining non-class attributes is empty \n",
      "\tor if all of the instances in D are in the same class\n",
      "    {\n",
      "\t\treturn an empty tree\n",
      "    }\n",
      "\telse \n",
      "    {\n",
      "\t\tcompute the Information Gain for each attribute over the dataset D\n",
      "\t\t\tlet a* be the attribute with maximum Information Gain\t\n",
      "\n",
      "\t\tcreate a root node for a tree T; label it with a* \n",
      "\n",
      "\t\tfor each value b of attribute a* \n",
      "        {\n",
      "\t\t\tlet T(a*=b) be the tree computed recursively by ID3 on input (D|a*=b, A-a*, C), where:\n",
      "\t\t\t\tD|a*=b contains all instances of D for which a* has the value b\n",
      "\t\t\t\tA-a* consists of all attributes of A except a*\n",
      "\n",
      "\t\t\tattach T(a*=b) to the root of T as a subtree\n",
      "\t\t}\n",
      "\t\t\n",
      "\t\treturn the resulting decision tree T\n",
      "\t}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using a Simple dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, we create a simple example where we try to predict the usage of mobile phones by individuals based on their income, age, education and marital status. Each of the attributes have been categorized into 2 or 3 types. Let us create the training dataset and tabulate it first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# training data\n",
      "train_income=['Low','Medium','Low','High','Low','High','Medium','Medium','High','Low','Medium',\n",
      "'Medium','High','Low','Medium']\n",
      "\n",
      "train_age = ['Old','Young','Old','Young','Old','Young','Young','Old','Old','Old','Young','Old',\n",
      "'Old','Old','Young']\n",
      "\n",
      "train_education = ['University','College','University','University','University','College','College',\n",
      "'High School','University','High School','College','High School','University','High School','College']\n",
      "\n",
      "train_marital = ['Married','Single','Married','Single','Married','Single','Married','Single','Single',\n",
      "'Married','Married','Single','Single','Married','Married']\n",
      "\n",
      "train_usage = ['Low','Medium','Low','High','Low','Medium','Medium','Low','High','Low','Medium','Low',\n",
      "'High','Low','Medium']\n",
      "\n",
      "# print data\n",
      "print 'Training Data Table : \\n'\n",
      "print 'Income \\t\\t Age \\t\\t Education \\t\\t Marital Status \\t Usage'\n",
      "for i in xrange(len(train_income)):\n",
      "\tprint train_income[i]+' \\t\\t '+train_age[i]+' \\t\\t '+train_education[i]+' \\t\\t '+train_marital[i]+' \\t\\t '+train_usage[i]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to create a decision tree from the above training dataset. The first step for that is to encode the data into numeric values and bind them to Shogun's features and multiclass labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import ID3ClassifierTree, RealFeatures, MulticlassLabels\n",
      "from numpy import array, concatenate\n",
      "\n",
      "# encoding dictionary\n",
      "income = {'Low' : 1.0, 'Medium' : 2.0, 'High' : 3.0}\n",
      "age = {'Young' : 1.0, 'Old' : 2.0}\n",
      "education = {'High School' : 1.0, 'College' : 2.0, 'University' : 3.0}\n",
      "marital_status = {'Married' : 1.0, 'Single' : 2.0}\n",
      "usage = {'Low' : 1.0, 'Medium' : 2.0, 'High' : 3.0}\n",
      "\n",
      "\n",
      "# encode training data\n",
      "for i in xrange(len(train_income)):\n",
      "\ttrain_income[i] = income[train_income[i]]\n",
      "\ttrain_age[i] = age[train_age[i]]\n",
      "\ttrain_education[i] = education[train_education[i]]\n",
      "\ttrain_marital[i] = marital_status[train_marital[i]]\n",
      "\ttrain_usage[i] = usage[train_usage[i]]\n",
      "    \n",
      "# form Shogun feature matrix\n",
      "train_data = array([train_income, train_age, train_education, train_marital])\n",
      "train_feats = RealFeatures(train_data);\n",
      "\n",
      "# form Shogun multiclass labels\n",
      "labels = MulticlassLabels(array(train_usage));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we learn our decision tree using the features and labels created."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create ID3ClassifierTree object\n",
      "id3 = ID3ClassifierTree()\n",
      "\n",
      "# set labels\n",
      "id3.set_labels(labels)\n",
      "\n",
      "# learn the tree from training features\n",
      "is_successful = id3.train(train_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our decision tree is ready now and we want to use it to make some predictions over test data. So, let us create some test data examples first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test data\n",
      "test_income = ['Medium','Medium','Low','High','High']\n",
      "test_age = ['Old','Young','Old','Young','Old']\n",
      "test_education = ['University','College','High School','University','College']\n",
      "test_marital = ['Married','Single','Married','Single','Married']\n",
      "test_usage = ['Low','Medium','Low','High','High']\n",
      "\n",
      "# tabulate test data\n",
      "print 'Test Data Table : \\n'\n",
      "print 'Income \\t\\t Age \\t\\t Education \\t\\t Marital Status \\t Usage'\n",
      "for i in xrange(len(test_income)):\n",
      "\tprint test_income[i]+' \\t\\t '+test_age[i]+' \\t\\t '+test_education[i]+' \\t\\t '+test_marital[i]+' \\t\\t ?'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, as with training data, we encode our test dataset and bind it to Shogun features. Then, we apply our decision tree to the test examples to obtain the predicted labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# encode test data\n",
      "for i in xrange(len(test_income)):\n",
      "\ttest_income[i] = income[test_income[i]]\n",
      "\ttest_age[i] = age[test_age[i]]\n",
      "\ttest_education[i] = education[test_education[i]]\n",
      "\ttest_marital[i] = marital_status[test_marital[i]]\n",
      "\n",
      "# bind to shogun features    \n",
      "test_data = array([test_income, test_age, test_education, test_marital])\n",
      "test_feats = RealFeatures(test_data)\n",
      "\n",
      "# apply decision tree classification\n",
      "test_labels = id3.apply_multiclass(test_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally let us tabulate the results obtained and compare them with our intuitive predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output = test_labels.get_labels();\n",
      "output_labels=[0]*len(output)\n",
      "\n",
      "# decode back test data for printing\n",
      "for i in xrange(len(test_income)):\n",
      "\ttest_income[i]=income.keys()[income.values().index(test_income[i])]\n",
      "\ttest_age[i]=age.keys()[age.values().index(test_age[i])]\n",
      "\ttest_education[i]=education.keys()[education.values().index(test_education[i])]\n",
      "\ttest_marital[i]=marital_status.keys()[marital_status.values().index(test_marital[i])]\n",
      "\toutput_labels[i]=usage.keys()[usage.values().index(output[i])]\n",
      "\n",
      "# print output data\n",
      "print 'Final Test Data Table : \\n'\n",
      "print 'Income \\t Age \\t Education \\t Marital Status \\t Usage(predicted)'\n",
      "for i in xrange(len(test_income)):\n",
      "\tprint test_income[i]+' \\t '+test_age[i]+' \\t '+test_education[i]+' \\t '+test_marital[i]+' \\t\\t '+output_labels[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, do the predictions made by our decision tree match our inferences from training set? Yes! For example, from the training set we infer that the individual having low income has low usage and also all individuals going to college have medium usage. The decision tree predicts the same for both cases. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using a real dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We choose the [car evaluation dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html) as our real-world dataset. The [car.names](https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.c45-names) file of the dataset enumerates the class categories as well as the non-class attributes. Each car categorized into one of 4 classes : unacc, acc, good, vgood. Each car is judged using 6 attributes : buying, maint, doors, persons, lug_boot, safety. Each of these attributes can take 3-4 values. Let us first make a dictionary to encode strings to numeric values using information from cars.names file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# class attribute\n",
      "evaluation = {'unacc' : 1.0, 'acc' : 2.0, 'good' : 3.0, 'vgood' : 4.0}\n",
      "\n",
      "# non-class attributes\n",
      "buying = {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0}\n",
      "maint = {'vhigh' : 1.0, 'high' : 2.0, 'med' : 3.0, 'low' : 4.0}\n",
      "doors = {'2' : 1.0, '3' : 2.0, '4' : 3.0, '5more' : 4.0}\n",
      "persons = {'2' : 1.0, '4' : 2.0, 'more' : 3.0}\n",
      "lug_boot = {'small' : 1.0, 'med' : 2.0, 'big' : 3.0}\n",
      "safety = {'low' : 1.0, 'med' : 2.0, 'high' : 3.0}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let us read the file and form Shogun features and labels."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open('../../../../data/multiclass/categorical_dataset/car.data', 'r')\n",
      "\n",
      "features = []\n",
      "labels = []\n",
      "\n",
      "# read data from file and encode\n",
      "for line in f:\n",
      "    words = line.rstrip().split(',')\n",
      "    words[0] = buying[words[0]]\n",
      "    words[1] = maint[words[1]]\n",
      "    words[2] = doors[words[2]]\n",
      "    words[3] = persons[words[3]]\n",
      "    words[4] = lug_boot[words[4]]\n",
      "    words[5] = safety[words[5]]\n",
      "    words[6] = evaluation[words[6]]\n",
      "    features.append(words[0:6])\n",
      "    labels.append(words[6])\n",
      "\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the entire dataset, let us choose some test vectors to form our test dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import random, delete\n",
      "\n",
      "features = array(features)\n",
      "labels = array(labels)\n",
      "\n",
      "# number of test vectors\n",
      "num_test_vectors = 200;\n",
      "\n",
      "test_indices = random.randint(features.shape[0], size = num_test_vectors)\n",
      "test_features = features[test_indices]\n",
      "test_labels = labels[test_indices]\n",
      "\n",
      "# remove test vectors from training set\n",
      "features = delete(features,test_indices,0)\n",
      "labels = delete(labels,test_indices,0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next step is to train our decision tree using the training features and applying it to our test dataset to get predicted output classes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# shogun test features and labels\n",
      "test_feats = RealFeatures(test_features.T)\n",
      "test_labels = MulticlassLabels(test_labels)\n",
      "\n",
      "# method for id3 training and\n",
      "def ID3_routine(features, labels):\n",
      "\n",
      "    # Shogun train features and labels\n",
      "    train_feats = RealFeatures(features.T)\n",
      "    train_lab = MulticlassLabels(labels)\n",
      "\n",
      "    # create ID3ClassifierTree object\n",
      "    id3 = ID3ClassifierTree()\n",
      "\n",
      "    # set labels\n",
      "    id3.set_labels(train_lab)\n",
      "\n",
      "    # learn the tree from training features\n",
      "    id3.train(train_feats)\n",
      "\n",
      "    # apply to test dataset\n",
      "    output = id3.apply_multiclass(test_feats)\n",
      "    \n",
      "    return output\n",
      "\n",
      "output = ID3_routine(features, labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let us compare our predicted labels with test labels to find out the percentage error of our classification model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import MulticlassAccuracy\n",
      "\n",
      "# Shogun object for calculating multiclass accuracy\n",
      "accuracy = MulticlassAccuracy()\n",
      "print 'Accuracy : ' + str(accuracy.evaluate(output, test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the accuracy is moderately high. Thus our decision tree can evaluate any car given its features with a high success rate. As a final exercise, let us examine the effect of training dataset size on the accuracy of decision tree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of error rates for all training dataset sizes\n",
      "error_rate = []\n",
      "\n",
      "# number of error rate readings taken for each value of dataset size\n",
      "num_repetitions = 3\n",
      "\n",
      "# loop over training dataset size\n",
      "for i in range(500,1600,200):\n",
      "    indices = random.randint(features.shape[0], size = i)\n",
      "    train_features = features[indices]\n",
      "    train_labels = labels[indices]\n",
      "    \n",
      "    average_error = 0\n",
      "    for i in xrange(num_repetitions):\n",
      "        output = ID3_routine(train_features, train_labels)\n",
      "        average_error = average_error + (1-accuracy.evaluate(output, test_labels))\n",
      "        \n",
      "    error_rate.append(average_error/num_repetitions)\n",
      "\n",
      "# plot the error rates    \n",
      "import matplotlib.pyplot as pyplot\n",
      "% matplotlib inline\n",
      "from scipy.interpolate import interp1d\n",
      "from numpy import linspace, arange\n",
      "\n",
      "fig,axis = pyplot.subplots(1,1)\n",
      "x = arange(500,1600,200)\n",
      "f = interp1d(x, error_rate)\n",
      "\n",
      "xnew = linspace(500,1500,100)\n",
      "pyplot.plot(x,error_rate,'o',xnew,f(xnew),'-')\n",
      "pyplot.xlim([400,1600])\n",
      "pyplot.xlabel('training dataset size')\n",
      "pyplot.ylabel('Classification Error')\n",
      "pyplot.title('Decision Tree Performance')\n",
      "pyplot.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "NOTE : The above code snippet takes about half a minute to execute. Please wait patiently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the above plot, we see that error rate decreases steadily as we increase the training dataset size. Although in this case, the decrease in error rate is not very significant, in many datasets this decrease in error rate can be substantial."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "C4.5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The C4.5 algorithm is essentially an extension of the ID3 algorithm for decision tree learning. It has the additional capability of handling continuous attributes and attributes with missing values. The tree growing process in case of C4.5 is same as that of ID3 i.e. finding the best split at each node using the [information gain metric](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees). But in case of continuous attribute, the C4.5 algorithm has to perform the additional step of converting it to a two-value categorical attribute by splitting about a suitable threshold. This threshold is chosen in a way such that the resultant split produces maximum information gain. Let us start exploring Shogun's C4.5 algorithm API with a toy example. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using toy dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us consider a 3-class classification using 2 attributes. One of the attributes (say attribute X) is a 2-class categorical attribute depicted by values 1 and 2. The other attribute (say attribute Y) is a continuous attribute having values between 1 and 9. The simple rules of classification are as follows : if X=1 and Y $\\epsilon$ [1,5), data point belongs to class 1, if X=1 and Y $\\epsilon$ [5,9), data point belongs to class 2 and if X=2, data point belongs to class 3. Let us realize the toy dataset by plotting it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "from numpy import ones, zeros, random, concatenate\n",
      "% matplotlib inline\n",
      "\n",
      "# number of datapoints in each category\n",
      "ncat = 20\n",
      "\n",
      "# create attribute values and labels for class 1\n",
      "x = ones((1,ncat))\n",
      "y = 1+random.rand(1,ncat)*4\n",
      "lab = zeros(ncat)\n",
      "\n",
      "# add attribute values and labels for class 2\n",
      "x = concatenate((x,ones((1,ncat))),1)\n",
      "y = concatenate((y,5+random.rand(1,ncat)*4),1)\n",
      "lab = concatenate((lab,ones(ncat)))\n",
      "\n",
      "# add attribute values and labels for class 3\n",
      "x = concatenate((x,2*ones((1,ncat))),1)\n",
      "y = concatenate((y,1+random.rand(1,ncat)*8),1)\n",
      "lab = concatenate((lab,2*ones(ncat)))\n",
      "\n",
      "# create test data\n",
      "ntest = 20\n",
      "x_t = concatenate((ones((1,3*ntest/4)),2*ones((1,ntest/4))),1)\n",
      "y_t = 1+random.rand(1,ntest)*8\n",
      "\n",
      "# plot training data\n",
      "c = ['r','g','b']\n",
      "for i in range(3):\n",
      "    plt.scatter(x[0,lab==i],y[0,lab==i],color=c[i],marker='x',s=50)\n",
      "\n",
      "# plot test data\n",
      "plt.scatter(x_t[0,:],y_t[0,:],color='k',s=10,alpha=0.8)\n",
      "\n",
      "plt.xlabel('attribute X')\n",
      "plt.ylabel('attribute Y')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above plot the training data points are are marked with different colours of crosses where each colour corresponds to a particular label. The test data points are marked by black circles. For us it is a trivial task to assign correct colours (i.e. labels) to the black points. Let us see how accurately C4.5 assigns colours to these test points. First, we create shogun objects from the data given to us."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import RealFeatures, MulticlassLabels\n",
      "\n",
      "# form training feature matrix\n",
      "train_feats = RealFeatures(concatenate((x,y),0))\n",
      "\n",
      "# from training labels\n",
      "train_labels = MulticlassLabels(lab)\n",
      "\n",
      "# from test feature matrix\n",
      "test_feats = RealFeatures(concatenate((x_t,y_t),0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let us train a decision tree using the C4.5 algorithm. We need to create a Shogun C4.5 tree object and supply training features and training labels to it. We also need to specify which attribute is categorical and which is continuous. The attribute types can be specified using [set_feature_types](http://www.shogun-toolbox.org/doc/en/latest/classshogun_1_1CC45ClassifierTree.html#a6fed413e6eee6a4d798613ea34d14512) method through which all categorical attributes are set as True and continuous attributes as False. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import array\n",
      "from modshogun import C45ClassifierTree\n",
      "\n",
      "# steps in C4.5 Tree training bundled together in a python method\n",
      "def train_tree(feats,types,labels):\n",
      "    # C4.5 Tree object\n",
      "    tree = C45ClassifierTree()\n",
      "    # set labels\n",
      "    tree.set_labels(labels)\n",
      "    # supply attribute types\n",
      "    tree.set_feature_types(types)\n",
      "    # supply training matrix and train\n",
      "    tree.train(feats)\n",
      "    \n",
      "    return tree\n",
      "\n",
      "# specify attribute types X is categorical hence True, Y is continuous hence False\n",
      "feat_types = array([True,False])\n",
      "\n",
      "# get back trained tree\n",
      "C45Tree = train_tree(train_feats,feat_types,train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have trained the decision tree, we can use it to classify our test vectors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_data(tree,data):\n",
      "    # get classification labels\n",
      "    output = tree.apply_multiclass(data)   \n",
      "    # get classification certainty\n",
      "    output_certainty=tree.get_certainty_vector()\n",
      "    \n",
      "    return output,output_certainty\n",
      "\n",
      "out_labels,out_certainty = classify_data(C45Tree,test_feats)\n",
      "out_labels = out_labels.get_labels()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us use the output labels to colour our test data points to qualitatively judge the performance of the decision tree.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import int32\n",
      "\n",
      "# plot results\n",
      "for i in range(out_labels.size):\n",
      "    plt.scatter(x_t[0,i],y_t[0,i],color=c[int32(out_labels[i])],s=50*out_certainty[i])\n",
      "\n",
      "# plot training dataset for visual comparison\n",
      "for i in range(3):\n",
      "    plt.scatter(x[0,lab==i],y[0,lab==i],color=c[i],marker='x',s=30,alpha=0.7)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the decision tree trained using the C4.5 algorithm works almost perfectly in this toy dataset. Now let us try this algorithm on a real world dataset. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Example using a real dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section we will investigate that how accurately we can predict the species of an Iris flower using a C4.5 trained decision tree. In this example we will use petal length, petal width, sepal length and sepal width as our attributes to decide among 3 classes of Iris : Iris Setosa, Iris Versicolor and Iris Verginica. Let us start by suitably reading the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "from numpy import array\n",
      "\n",
      "# dictionary to encode class names to class labels\n",
      "to_label = {'Iris-setosa' : 0.0, 'Iris-versicolor' : 1.0, 'Iris-virginica' : 2.0}\n",
      "\n",
      "# read csv file and separate out labels and features\n",
      "lab = []\n",
      "feat = []\n",
      "with open('../../../../data/multiclass/iris.data') as csvfile:\n",
      "    csvread = csv.reader(csvfile,delimiter=',')\n",
      "    for row in csvread:\n",
      "        feat.append([float(i) for i in row[0:4]])\n",
      "        lab.append(to_label[row[4]])\n",
      "\n",
      "lab = array(lab)\n",
      "feat = array(feat).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because there is no separate test dataset, we first divide the given dataset into training and testing subsets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import int32, random\n",
      "\n",
      "# no.of vectors in test dataset\n",
      "ntest = 25\n",
      "# no. of vectors in train dataset\n",
      "ntrain = 150-ntest\n",
      "\n",
      "# randomize the order of vectors\n",
      "subset = int32(random.permutation(150))\n",
      "\n",
      "# choose 1st ntrain from randomized set as training vectors\n",
      "feats_train = feat[:,subset[0:ntrain]]\n",
      "# form training labels correspondingly\n",
      "train_labels = lab[subset[0:ntrain]]\n",
      "\n",
      "# form test features and labels (for accuracy evaluations)\n",
      "feats_test = feat[:,subset[ntrain:ntrain+ntest]]\n",
      "test_labels = lab[subset[ntrain:ntrain+ntest]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before marching forward with applying C4.5, let us plot the data to get a better understanding. The given data points are 4-D and hence cannot be conveniently plotted. We need to reduce the number of dimensions to 2. This reduction can be achieved using any dimension reduction algorithm like PCA. However for the sake of brevity, let us just choose two highly correlated dimensions, petal width and petal length (see [summary statistics](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names)), right away for plotting.   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "% matplotlib inline\n",
      "\n",
      "# plot training features\n",
      "c = ['r', 'g', 'b']\n",
      "for i in range(3):\n",
      "    plt.scatter(feats_train[2,train_labels==i],feats_train[3,train_labels==i],color=c[i],marker='x')\n",
      "\n",
      "# plot test data points in black\n",
      "plt.scatter(feats_test[2,:],feats_test[3,:],color='k',marker='o')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let us create Shogun features and labels from the given data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import RealFeatures, MulticlassLabels\n",
      "\n",
      "# training data\n",
      "feats_train = RealFeatures(feats_train)\n",
      "train_labels = MulticlassLabels(train_labels)\n",
      "\n",
      "# test data\n",
      "feats_test = RealFeatures(feats_test)\n",
      "test_labels = MulticlassLabels(test_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know for fact that decision trees tend to overfit. Hence pruning becomes a necessary step. In case of toy dataset, we skipped the pruning step because the dataset was simple and noise-free. But in case of a real dataset like the Iris dataset pruning cannot be skipped. So we have to partition the training dataset into the training subset and the validation subset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# randomize the order of vectors\n",
      "subset = int32(random.permutation(ntrain))\n",
      "\n",
      "nvalidation = 45\n",
      "\n",
      "# form training subset and validation subset\n",
      "train_subset = subset[0:ntrain-nvalidation]\n",
      "validation_subset = subset[ntrain-nvalidation:ntrain]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we train the decision tree first, then prune it and finally use it to get output labels for test vectors. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set attribute types - all continuous\n",
      "feature_types = array([False, False, False, False])\n",
      "\n",
      "# remove validation subset before training the tree\n",
      "feats_train.add_subset(train_subset)\n",
      "train_labels.add_subset(train_subset)\n",
      "\n",
      "# train tree\n",
      "C45Tree = train_tree(feats_train,feature_types,train_labels)\n",
      "\n",
      "# bring back validation subset\n",
      "feats_train.remove_subset()\n",
      "train_labels.remove_subset()\n",
      "\n",
      "# remove data belonging to training subset\n",
      "feats_train.add_subset(validation_subset)\n",
      "train_labels.add_subset(validation_subset)\n",
      "\n",
      "# prune the tree\n",
      "C45Tree.prune_tree(feats_train,train_labels)\n",
      "\n",
      "# bring back training subset\n",
      "feats_train.remove_subset()\n",
      "train_labels.remove_subset()\n",
      "\n",
      "# get results\n",
      "output, output_certainty = classify_data(C45Tree,feats_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us calculate the accuracy of the classification made by our tree as well as plot the results for qualitative evaluation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from modshogun import MulticlassAccuracy\n",
      "\n",
      "# Shogun object for calculating multiclass accuracy\n",
      "accuracy = MulticlassAccuracy()\n",
      "print 'Accuracy : ' + str(accuracy.evaluate(output, test_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert MulticlassLabels object to labels vector\n",
      "output = output.get_labels()\n",
      "test_labels = test_labels.get_labels()\n",
      "train_labels = train_labels.get_labels()\n",
      "\n",
      "# convert RealFeatures object to matrix\n",
      "feats_test = feats_test.get_feature_matrix()\n",
      "feats_train = feats_train.get_feature_matrix()\n",
      "\n",
      "# plot ground truth\n",
      "for i in range(3):\n",
      "    plt.scatter(feats_test[2,test_labels==i],feats_test[3,test_labels==i],color=c[i],marker='x',s=100)\n",
      "\n",
      "# plot predicted labels\n",
      "for i in range(output.size):\n",
      "    plt.scatter(feats_test[2,i],feats_test[3,i],color=c[int32(output[i])],marker='o',s=30*output_certainty[i])\n",
      "        \n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the evaluation of results, we infer that, with the help of a C4.5 trained decision tree, we can predict (with high accuracy) the type of Iris plant given its petal and sepal widths and lengths."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "References"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science\n",
      "\n",
      "[2] Quinlan, J. R. 1986. Induction of Decision Trees. Mach. Learn. 1: 1 (Mar. 1986), 81-106"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}