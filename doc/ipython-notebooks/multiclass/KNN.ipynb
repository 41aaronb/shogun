{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*K-Nearest Neighbors* (KNN) is a very simple and effectiv algorithm. The training of a KNN model basically does nothing but memorizing all the training points and the associated labels, which is very cheap in computation but costly in storage. The prediction is implemented by finding the K nearest neighbors of the query point, and voting. Here K is a hyper-parameter for the algorithm. Smaller K gives the model low bias but high variance; while larger K gives low variance but high bias.\n",
      "\n",
      "In `SHOGUN`, you can use `CKNN` to perform KNN learning. To construct a KNN machine, you must choose the hyper-parameter K and a distance function. Usually, we simply use the standard  `CEuclideanDistance`, but in general, any subclass of `CDistance` could be used. For demonstration, in this tutorial we select a random subset of 1000 samples from the USPS digit recognition dataset, and run 2-fold cross validation of KNN with varying K."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In `SHOGUN`, you can use *Cover Tree* to speed up the nearest neighbor searching process in KNN. Just call `set_use_covertree` on the KNN machine to enable or disable this feature. We also show the prediction time comparison with and without Cover Tree in this tutorial."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we load and init data split:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "from scipy.io import loadmat, savemat\n",
      "from numpy    import random\n",
      "from os       import path\n",
      "\n",
      "mat  = loadmat('../../../data/multiclass/usps.mat')\n",
      "Xall = mat['data']\n",
      "Yall = mat['label'].squeeze()\n",
      "\n",
      "# map from 1..10 to 0..9, since shogun\n",
      "# requires multiclass labels to be\n",
      "# 0, 1, ..., K-1\n",
      "Yall = Yall - 1\n",
      "\n",
      "random.seed(0)\n",
      "\n",
      "subset = random.permutation(len(Yall))[:1000]\n",
      "Xall = Xall[:, subset]\n",
      "Yall = Yall[subset]\n",
      "\n",
      "Nsplit = 2\n",
      "all_ks = range(1, 21)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we import shogun components and convert the data to shogun objects:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "from modshogun import MulticlassLabels, CrossValidationSplitting\n",
      "from modshogun import MulticlassAccuracy\n",
      "from modshogun import KNN\n",
      "from modshogun import EuclideanDistance\n",
      "from modshogun import RealFeatures\n",
      "\n",
      "labels = MulticlassLabels(Yall.astype('d'))\n",
      "feats  = RealFeatures(Xall)\n",
      "\n",
      "split = CrossValidationSplitting(labels, Nsplit)\n",
      "split.build_subsets()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A helper function is defined to run evaluation for KNN:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate(use_cover_tree=False):\n",
      "    accuracy = np.zeros((Nsplit, len(all_ks)))\n",
      "    acc_train = np.zeros(accuracy.shape)\n",
      "    time_test = np.zeros(accuracy.shape)\n",
      "    for i in range(Nsplit):\n",
      "        idx_train = split.generate_subset_inverse(i)\n",
      "        idx_test  = split.generate_subset_indices(i)\n",
      "\n",
      "        for j, k in enumerate(all_ks):\n",
      "            #print \"Round %d for k=%d...\" % (i, k)\n",
      "\n",
      "            feats.add_subset(idx_train)\n",
      "            labels.add_subset(idx_train)\n",
      "\n",
      "            dist = EuclideanDistance(feats, feats)\n",
      "            knn = KNN(k, dist, labels)\n",
      "            knn.set_store_model_features(True)\n",
      "            knn.set_use_covertree(use_cover_tree)\n",
      "            knn.train()\n",
      "\n",
      "            evaluator = MulticlassAccuracy()\n",
      "            pred = knn.apply_multiclass()\n",
      "            acc_train[i, j] = evaluator.evaluate(pred, labels)\n",
      "\n",
      "            feats.remove_subset()\n",
      "            labels.remove_subset()\n",
      "            feats.add_subset(idx_test)\n",
      "            labels.add_subset(idx_test)\n",
      "\n",
      "            t_start = time.clock()\n",
      "            pred = knn.apply_multiclass(feats)\n",
      "            time_test[i, j] = (time.clock() - t_start) / labels.get_num_labels()\n",
      "\n",
      "            accuracy[i, j] = evaluator.evaluate(pred, labels)\n",
      "\n",
      "            feats.remove_subset()\n",
      "            labels.remove_subset()\n",
      "    return {'eout': accuracy, 'ein': acc_train, 'time': time_test}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluate KNN with and without Cover Tree. This takes a few seconds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Evaluating KNN...\")\n",
      "wo_ct = evaluate()\n",
      "wi_ct = evaluate(use_cover_tree=True)\n",
      "print(\"Done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generate plots with the data collected in the evaluation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib\n",
      "\n",
      "import pylab as P\n",
      "\n",
      "fig = P.figure(figsize=(8,5))\n",
      "P.plot(all_ks, wo_ct['eout'].mean(axis=0), 'r-*')\n",
      "P.plot(all_ks, wo_ct['ein'].mean(axis=0), 'r--*')\n",
      "P.legend([\"Test Accuracy\", \"Training Accuracy\"])\n",
      "P.xlabel('K')\n",
      "P.ylabel('Accuracy')\n",
      "P.title('KNN Accuracy')\n",
      "P.tight_layout()\n",
      "\n",
      "fig = P.figure(figsize=(8,5))\n",
      "P.plot(all_ks, wo_ct['time'].mean(axis=0), 'r-*')\n",
      "P.plot(all_ks, wi_ct['time'].mean(axis=0), 'b-d')\n",
      "P.xlabel(\"K\")\n",
      "P.ylabel(\"time\")\n",
      "P.title('KNN time')\n",
      "P.legend([\"Plain KNN\", \"CoverTree KNN\"], loc='center right')\n",
      "P.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although simple and elegant, KNN is generally very resource costly. Because all the training samples are to be memorized literally, the memory cost of KNN *learning* becomes prohibitive when the dataset is huge. Even when the memory is big enough to hold all the data, the prediction will be slow, since the distances between the query point and all the training points need to be computed and ranked. The situation becomes worse if in addition the data samples are all very high-dimensional."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}